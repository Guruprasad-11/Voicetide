{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS47CQzj3nUq"
      },
      "source": [
        "# YouTube Comments Analysis Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook provides an **experimental pipeline** for extracting, translating, analyzing sentiment, and summarizing YouTube video comments using the YouTube Data API v3.\n",
        "\n",
        "### Features\n",
        "- Fetch top-level comments and replies from a YouTube video.\n",
        "- Translate non-English comments to English using Google Translate.\n",
        "- Perform sentiment analysis using TextBlob.\n",
        "- Summarize viewer opinions and extract common topics/suggestions.\n",
        "- Export results to a CSV file and generate a summary report.\n",
        "\n",
        "### Setup Instructions (Recommended for Google Colab)\n",
        "\n",
        "1. **Use Google Colab**:\n",
        "   - Click the \"Open in Colab\" button (or upload this notebook to [https://colab.research.google.com](https://colab.research.google.com)).\n",
        "\n",
        "2. **Add Your YouTube API Key**:\n",
        "   - In the `main()` function, replace the `api_key` value with your **YouTube Data API v3 key**:\n",
        "     ```python\n",
        "     api_key = \"YOUR_YOUTUBE_API_KEY\"\n",
        "     ```\n",
        "\n",
        "3. **Set the Target YouTube Video**:\n",
        "   - Replace `video_id` with the ID of the video you want to analyze.\n",
        "   - You can find it at the end of any YouTube URL:\n",
        "     ```\n",
        "     Example:\n",
        "     URL: https://www.youtube.com/watch?v=2MWQZ4CVbks\n",
        "     â†’ video_id = \"2MWQZ4CVbks\"\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "> Note: This notebook accesses online data from YouTube. It may take some time depending on the number of comments and replies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcd96h5u8HR0"
      },
      "source": [
        "###  Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYDk_A2H8KSL"
      },
      "outputs": [],
      "source": [
        "!pip install google-api-python-client pandas textblob googletrans==4.0.0-rc1 nltk gensim sumy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbSzVtLgBmkU"
      },
      "source": [
        "### Import Libraries and Load Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-pudp_R8NXD"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import traceback\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYcwCDfVBi_s"
      },
      "source": [
        "### Function to Fetch Comments from YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQtK3braAuJ_"
      },
      "outputs": [],
      "source": [
        "def get_comments(api_key, video_id):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet,replies\",\n",
        "        videoId=video_id,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "\n",
        "    # Create empty DataFrame to store comments\n",
        "    df = pd.DataFrame(columns=['comment', 'replies', 'date', 'user_name'])\n",
        "\n",
        "    while request:\n",
        "        comments, replies, dates, user_names = [], [], [], []\n",
        "\n",
        "        try:\n",
        "            response = request.execute()\n",
        "\n",
        "            for item in response['items']:\n",
        "                comment_data = item['snippet']['topLevelComment']['snippet']\n",
        "                comments.append(comment_data['textDisplay'])\n",
        "                user_names.append(comment_data['authorDisplayName'])\n",
        "                dates.append(comment_data['publishedAt'])\n",
        "\n",
        "                reply_list = [\n",
        "                    reply['snippet']['textDisplay']\n",
        "                    for reply in item.get('replies', {}).get('comments', [])\n",
        "                ]\n",
        "                replies.append(reply_list)\n",
        "\n",
        "            # Append the new data to the main DataFrame\n",
        "            df2 = pd.DataFrame({\n",
        "                \"comment\": comments,\n",
        "                \"replies\": replies,\n",
        "                \"user_name\": user_names,\n",
        "                \"date\": dates\n",
        "            })\n",
        "\n",
        "            df = pd.concat([df, df2], ignore_index=True)\n",
        "            df.to_csv(f\"{video_id}_user_comments.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "            request = youtube.commentThreads().list_next(request, response)\n",
        "            print(\"Fetched next page of comments...\")\n",
        "            sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\\n{traceback.format_exc()}\")\n",
        "            print(\"Pausing for 10 seconds...\")\n",
        "            sleep(10)\n",
        "            df.to_csv(f\"{video_id}_user_comments.csv\", index=False, encoding='utf-8')\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raiLMHsZBcEV"
      },
      "source": [
        "### Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RwP1v2XAxx-"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    api_key = # YOUR_API_KEY\n",
        "    video_id = # YOUR_VIDEO_ID\n",
        "    get_comments(api_key, video_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocJ0EMLDBZIN"
      },
      "source": [
        "### Import NLP Libraries and Download NLTK Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8YcXmwWAzSe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from googletrans import Translator\n",
        "import re\n",
        "from collections import Counter\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTgvf0eVBVFt"
      },
      "source": [
        "### Text Preprocessing Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FglsulsA0kG"
      },
      "outputs": [],
      "source": [
        "def load_data(csv_file):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        if 'comment' not in df.columns:\n",
        "            raise ValueError(\"CSV must contain a 'comment' column.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r'@\\w+|#', '', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "    return text.lower().strip()\n",
        "\n",
        "def translate_text(text, target_lang=\"en\"):\n",
        "    translator = Translator()\n",
        "    try:\n",
        "        detected = translator.detect(text).lang\n",
        "        if detected != target_lang:\n",
        "            return translator.translate(text, dest=target_lang).text\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTegVzUrBRnd"
      },
      "source": [
        "### Sentiment Analysis and Suggestion Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRmRvtqQA3AG"
      },
      "outputs": [],
      "source": [
        "def analyze_sentiment(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    return \"Positive\" if polarity > 0 else \"Negative\" if polarity < 0 else \"Neutral\"\n",
        "\n",
        "def extract_suggestions(df):\n",
        "    keywords = [\"should\", \"make\", \"video on\", \"talk about\", \"cover\", \"do a video on\"]\n",
        "    return [comment for comment in df['translated_comment'] if any(k in comment for k in keywords)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eajrdlGBNht"
      },
      "source": [
        "### Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9NdFQF9A4G-"
      },
      "outputs": [],
      "source": [
        "def summarize_text(text, num_sentences=5):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = TextRankSummarizer()\n",
        "    return \" \".join(str(sentence) for sentence in summarizer(parser.document, num_sentences))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZar4QVkBEvV"
      },
      "source": [
        "### Generate Full Comment Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3c4gsMgA5K-"
      },
      "outputs": [],
      "source": [
        "def generate_summary(df):\n",
        "    if df is None:\n",
        "        return \"No data to summarize.\"\n",
        "\n",
        "    df['cleaned_comment'] = df['comment'].astype(str).apply(clean_text)\n",
        "    df['translated_comment'] = df['cleaned_comment'].apply(translate_text)\n",
        "    df['sentiment'] = df['translated_comment'].apply(analyze_sentiment)\n",
        "\n",
        "    sentiment_counts = df['sentiment'].value_counts()\n",
        "    suggestions = extract_suggestions(df)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = \" \".join(df['translated_comment']).split()\n",
        "    common_words = Counter(w for w in words if w not in stop_words).most_common(10)\n",
        "\n",
        "    text = \". \".join(df['translated_comment'])\n",
        "    summary_text = summarize_text(text) if len(text) > 200 else \"Not enough text to summarize.\"\n",
        "\n",
        "    return f\"\"\"\n",
        "    **YouTube Comments Summary**\n",
        "\n",
        "    - **Total Comments:** {len(df)}\n",
        "    - **Positive:** {sentiment_counts.get(\"Positive\", 0)}\n",
        "    - **Negative:** {sentiment_counts.get(\"Negative\", 0)}\n",
        "    - **Neutral:** {sentiment_counts.get(\"Neutral\", 0)}\n",
        "\n",
        "    **Audience Summary:**\n",
        "    {summary_text}\n",
        "\n",
        "    **Viewer Suggestions:**\n",
        "    {suggestions[:5] if suggestions else \"No major suggestions found.\"}\n",
        "\n",
        "    **Common Words:**\n",
        "    {', '.join(word for word, _ in common_words)}\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc6aqA2PA-NF"
      },
      "source": [
        "### Run Summary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeNnaynZA7L-"
      },
      "outputs": [],
      "source": [
        "csv_file = \"/content/comments.csv\"  # Adjust path as needed\n",
        "df = load_data(csv_file)\n",
        "summary_result = generate_summary(df)\n",
        "print(summary_result)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
